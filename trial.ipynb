{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth=3\n",
    "\n",
    "learning_rate=0.1\n",
    "\n",
    "n_estimators=100\n",
    "\n",
    "silent=True\n",
    "\n",
    "objective='binary:logistic'\n",
    "\n",
    "booster='gbtree'\n",
    "\n",
    "n_jobs=1\n",
    "\n",
    "nthread=None\n",
    "\n",
    "gamma=0\n",
    "\n",
    "min_child_weight=1\n",
    "\n",
    "max_delta_step=0\n",
    "\n",
    "subsample=1\n",
    "\n",
    "colsample_bytree=1\n",
    "\n",
    "colsample_bylevel=1\n",
    "\n",
    "reg_alpha=0\n",
    "\n",
    "reg_lambda=1\n",
    "\n",
    "scale_pos_weight=1\n",
    "\n",
    "base_score=0.5\n",
    "\n",
    "random_state=0\n",
    "\n",
    "seed=None\n",
    "\n",
    "missing=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_binary_classifier(train=None,\n",
    "                        tests=[],\n",
    "                        test_file_names=[],\n",
    "                        target=None,\n",
    "                        model_name='model',\n",
    "                        index_col=None,\n",
    "                        parameter_optimization=True,\n",
    "                        show_n_report=5,\n",
    "                        show_feature_importance=True,\n",
    "                        param_grid={  \n",
    "                                    \"learning_rate\":[.01,.015,.02,.025,.03,.035,.04,.045,.05],\n",
    "                                    \"gamma\":[i/10.0 for i in range(0,5)],\n",
    "                                    \"max_depth\": [2,3,4,5,6,7,8],\n",
    "                                    \"min_child_weight\":[1,2,5,10],\n",
    "                                    \"max_delta_step\":[0,1,2,5,10],\n",
    "                                    \"subsample\":[i/10.0 for i in range(5,10)],\n",
    "                                    \"colsample_bytree\":[i/10.0 for i in range(5,10)],\n",
    "                                    \"colsample_bylevel\":[i/10.0 for i in range(5,10)],\n",
    "                                    \"reg_lambda\":[1e-5, 1e-2, 0.1, 1, 100], \n",
    "                                    \"reg_alpha\":[1e-5, 1e-2, 0.1, 1, 100],\n",
    "                                    \"scale_pos_weight\":[1,2,3,4,5,6,7,8,9,10,20,30,40],\n",
    "                                    \"n_estimators\":[100,500,700,1000]},\n",
    "                        n_iter=10,\n",
    "                        CV=5,\n",
    "                        scoring='roc_auc',\n",
    "                        best_model=True,\n",
    "                        learning_rate=0.1,\n",
    "                        gamma=0,\n",
    "                        max_depth=3,\n",
    "                        min_child_weight=1,\n",
    "                        max_delta_step=0,\n",
    "                        subsample=1,\n",
    "                        colsample_bytree=1,\n",
    "                        colsample_bylevel=1,\n",
    "                        reg_lambda=1,\n",
    "                        reg_alpha=0,\n",
    "                        scale_pos_weight=1,\n",
    "                        n_estimators=100,\n",
    "                        objective='binary:logistic'\n",
    "                                     ):\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ====================================== defining required functions =====================\n",
    "    def get_logger(name):\n",
    "        import logging\n",
    "        log_format = f\"%(asctime)s  %(name)8s  %(levelname)5s  %(message)s at line no %(lineno)d\"\n",
    "        logging.basicConfig(level=logging.DEBUG,\n",
    "                            format=log_format,\n",
    "                            filename='log.log',\n",
    "                            filemode='w')\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.DEBUG)\n",
    "        console.setFormatter(logging.Formatter(log_format))    \n",
    "        logging.getLogger(name).addHandler(console)\n",
    "        return logging.getLogger(name)\n",
    "    # ----------------------------------gain function ----------------------------------\n",
    "    \n",
    "    def createGains(test,dep_var):\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns \n",
    "        import warnings\n",
    "        \"\"\"\n",
    "    createGains - A function to create Gains table and plot gains in Python\n",
    "    =======================================================================================\n",
    "\n",
    "    **createGains** is a Python function by MathMarket (Team Tesla) of TheMathCompany,\n",
    "    It aims to create Gains for the model built dataset\n",
    "\n",
    "    This function returns Gains table at index 0 and plot at index 1\n",
    "\n",
    "    The module is built using matplotlib,pandas.\n",
    "    Therefore, these libraries are need to be installed in order to use the module.\n",
    "\n",
    "    The module consist of one function:\n",
    "    `createGains(test dataset, dependent variable)`\n",
    "\n",
    "    \"\"\"\n",
    "       \n",
    "        try:\n",
    "            warnings.filterwarnings('ignore')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            test = test.sort_values(by='Pred_Prob',ascending=False)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Could not sort according to predicted probability.\")\n",
    "        try:\n",
    "            test['bucket'] = pd.qcut(test.Pred_Prob, 10,duplicates='drop')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Could not create buckets of ten.\")\n",
    "        try:\n",
    "            test['rank'] = test['Pred_Prob'].rank(method='first')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            test['bucket'] = pd.qcut(test['rank'].values, 10).codes\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            test[\"DepVar\"]=test[dep_var]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            test[\"Non_Event\"]=np.where(test[\"DepVar\"] == 0 , 1,0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            grouped = test.groupby('bucket', as_index = True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg1 = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg1['min_score']=grouped.min().Pred_Prob\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:    \n",
    "            agg1['max_scr'] = grouped.max().Pred_Prob\n",
    "        except Exception as e:\n",
    "            print(e)    \n",
    "        try:\n",
    "            agg1['DepVar'] = grouped.sum().DepVar\n",
    "        except Exception as e:\n",
    "            print(e)    \n",
    "        try:\n",
    "            agg1['Non_Event'] = grouped.sum().Non_Event\n",
    "        except Exception as e:\n",
    "            print(e)    \n",
    "        try:    \n",
    "            agg1['total'] = agg1.DepVar + agg1.Non_Event\n",
    "        except Exception as e:\n",
    "            print(e)    \n",
    "        try:    \n",
    "            agg2=agg1.iloc[: : -1]\n",
    "        except Exception as e:\n",
    "            print(e)    \n",
    "        try:    \n",
    "            pd.options.mode.chained_assignment = None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2.is_copy\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2['Cumulative_Sum']=agg2['DepVar'].cumsum()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2['Perc_of_Events']= (agg2.DepVar/agg2.DepVar.sum()) * 100\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2['Gain']=agg2['Perc_of_Events'].cumsum()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2.reset_index(drop=True, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2[\"Bucket\"]=[1,2,3,4,5,6,7,8,9,10]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2.min_score = agg2.min_score.round(3)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2.max_scr = agg2.max_scr.round(3)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2.Perc_of_Events = agg2.Perc_of_Events.round(3)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            agg2.Gain = agg2.Gain.round(3)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            cols = ['min_score','max_scr','DepVar','Non_Event','total','Cumulative_Sum','Perc_of_Events','Gain','Bucket']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            df2 = pd.DataFrame(columns=cols, index=range(1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            for a in range(1):\n",
    "                df2.loc[a].min_score = 0\n",
    "                df2.loc[a].max_scr = 0\n",
    "                df2.loc[a].DepVar = 0\n",
    "                df2.loc[a].Non_Event = 0\n",
    "                df2.loc[a].total = 0\n",
    "                df2.loc[a].Cumulative_Sum = 0\n",
    "                df2.loc[a].Perc_of_Events = 0\n",
    "                df2.loc[a].Gain = 0\n",
    "                df2.loc[a].Bucket= 0\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            frames=[df2,agg2]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            result = pd.concat(frames,ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Could not generate gains table. Check if function parameters are correct and previous cells are executed\")\n",
    "        try:\n",
    "            figgains, ax1 = plt.subplots()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax2 = ax1.twinx()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            line_up, = ax1.plot(result['Gain'], 'o-',color=\"#ED7038\",linewidth=2,label=\"Gain\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            line_down, = ax2.plot(result['Bucket'],'k--',linewidth=2,label=\"Random\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax1.grid(b=True, which='both', color='0.65',linestyle='-')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax1.set_xlabel('Buckets')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax1.set_ylabel(\"Gain with model\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax2.set_ylabel('Random gain')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            plt.title('Gains Chart')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            plt.legend(handles=[line_up, line_down],loc = \"lower right\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax2.set_yticklabels([\"0\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\"])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            warnings.filterwarnings('ignore')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            return(result,figgains)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error: Could not generate gains Plot. Check if function parameters are correct and previous cells are executed\")\n",
    "\n",
    "\n",
    "    # -----------------------------------function to get KS-----------------------------------\n",
    "    def getKS(test,dep_var):\n",
    "        import warnings\n",
    "        import matplotlib.pyplot as plt\n",
    "        \"\"\"\n",
    "    getKS - A function to get KS-value and plot KS in Python\n",
    "    =======================================================================================\n",
    "\n",
    "    **getKS** is a Python function by MathMarket (Team Tesla) of TheMathCompany,\n",
    "    It aims to create KS-value and plot KS.\n",
    "\n",
    "    This function returns KS-value at index 0 and plot at index 1\n",
    "\n",
    "    The module is built using matplotlib,pandas.\n",
    "    Therefore, these libraries are need to be installed in order to use the module.\n",
    "\n",
    "    The module consist of one function:\n",
    "    `getKS(test dataset, dependent variable)`\n",
    "    \"\"\"    \n",
    "        try:\n",
    "            warnings.filterwarnings('ignore')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks = test.sort_values(by='Pred_Prob',ascending=False)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Could not sort dataframe according to predicted probability\")\n",
    "        try:\n",
    "            ks['bucket'] = pd.qcut(test.Pred_Prob, 10,duplicates='drop')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks['rank'] = ks['Pred_Prob'].rank(method='first')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks['bucket'] = pd.qcut(ks['rank'].values, 10).codes\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks[\"Event\"]=test[dep_var]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks['NonEvent']=np.where(ks[dep_var] == 0 , 1,0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            grouped = ks.groupby('bucket', as_index = True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table1 = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:    \n",
    "            ks_table1['min_score']=grouped.min().Pred_Prob\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table1['max_scr'] = grouped.max().Pred_Prob\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table1['Event'] = grouped.sum().Event\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table1['NonEvent'] = grouped.sum().NonEvent\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table1['total'] = ks_table1.Event + ks_table1.NonEvent\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2=ks_table1.iloc[: : -1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            pd.options.mode.chained_assignment = None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2.is_copy\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2['Cumulative_Sum_Event']=ks_table2['Event'].cumsum()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2['Perc_of_Events']= (ks_table2.Event/ks_table2.Event.sum()) * 100\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2['Cumulative_Percent_Event']=ks_table2['Perc_of_Events'].cumsum()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2['Cumulative_Sum_NonEvent']=ks_table2['NonEvent'].cumsum()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2['Perc_of_NonEvents']= (ks_table2.NonEvent/ks_table2.NonEvent.sum()) * 100\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2['Cumulative_Percent_NonEvent']=ks_table2['Perc_of_NonEvents'].cumsum()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table2['ks'] = np.round(ks_table2.Cumulative_Percent_Event-ks_table2.Cumulative_Percent_NonEvent)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            cols = ['min_score','max_scr','Event','Non_Event','total','Cumulative_Sum_Event','Perc_of_Events','Cumulative_Percent_Event','Cumulative_Sum_NonEvent','Perc_of_NonEvents','Cumulative_Percent_NonEvent','ks']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            df2 = pd.DataFrame(columns=cols, index=range(1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            for a in range(1):\n",
    "                df2.loc[a].min_score = 0\n",
    "                df2.loc[a].max_scr = 0\n",
    "                df2.loc[a].DepVar = 0\n",
    "                df2.loc[a].Non_Event = 0\n",
    "                df2.loc[a].total = 0\n",
    "                df2.loc[a].Cumulative_Sum_Event = 0\n",
    "                df2.loc[a].Perc_of_Events = 0\n",
    "                df2.loc[a].Cumulative_Percent_Event = 0\n",
    "                df2.loc[a].Cumulative_Sum_NonEvent= 0\n",
    "                df2.loc[a].Perc_of_NonEvents= 0 \n",
    "                df2.loc[a].Cumulative_Percent_NonEvent= 0\n",
    "                df2.loc[a].ks = 0\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            frames=[df2,ks_table2]  \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            result = pd.concat(frames,ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table = pd.DataFrame(ks_table2)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table[\"Bucket\"]=[1,2,3,4,5,6,7,8,9,10]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table = ks_table.set_index('Bucket')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ksvalue = ks_table.ks.max()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ksvalue = round(ksvalue,2)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error: Could not generate Ks table. Check if function parameters are corNon_Eventt and previous cells are executed\")\n",
    "        try:\n",
    "            figks, ax1 = plt.subplots()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax2 = ax1.twinx()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            line_down, = ax2.plot(result['Cumulative_Percent_NonEvent'],'k-',linewidth=2,label=\"Non event\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            line_up, = ax1.plot(result['Cumulative_Percent_Event'], 'g-',color=\"#ED7038\",linewidth=2,label=\"Event\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:    \n",
    "            ax1.grid(b=True, which='both', color='0.65',linestyle='-')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax1.set_xlabel('Buckets')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax1.set_ylabel(\"Event\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax2.set_ylabel('Non Event')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            plt.title('K-S Chart')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ksvalue = str(ksvalue)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            bbox = dict(boxstyle=\"round\", fc=\"white\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            plt.annotate(\"KS: \"+ksvalue, xy=(9.6,22), ha='center', va='center',bbox=bbox)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            plt.legend(handles=[line_up, line_down],loc = \"lower right\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ax2.set_yticklabels([])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            flag = lambda x: '<----(max)' if x == result.ks.max() else ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ks_table['max_ks'] = result.ks.apply(flag)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            print(ks_table)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            warnings.filterwarnings('ignore')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            return(ks_table,figks,{'KS_Value':ksvalue})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error: Could not generate KS. Check if function parameters are corNon_Eventt and previous cells are executed\")\n",
    "\n",
    "    #----------------------------- report function ---------------------\n",
    "    def report(results, n_top=3):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.5f} (std: {1:.5f})\".format(\n",
    "                    results['mean_test_score'][candidate],\n",
    "                    results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "\n",
    "    # -------------------------- function to get concordance -----------------------\n",
    "    def getConcordance(test,dep_var):\n",
    "        from bisect import bisect_left,bisect_right\n",
    "        \"\"\"\n",
    "    getConcordance - A function to create concordance, discordance, tie pairs in Python\n",
    "    =========================================================================================\n",
    "\n",
    "    **getConcordance** is a Python function by MathMarket (Team Tesla) of TheMathCompany,\n",
    "\n",
    "    The module consist of one function:\n",
    "    `getConcordance(test dataset, dependent variable)`\n",
    "    \"\"\"    \n",
    "        try:\n",
    "            zeros = test[(test[dep_var]==0)].reset_index().drop(['index'], axis = 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ones = test[(test[dep_var]==1)].reset_index().drop(['index'], axis = 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            pred_prob = \"Pred_Prob\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            zeros2 = zeros[[dep_var,pred_prob]].copy()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ones2=ones[[dep_var,pred_prob]].copy()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            zeros2_list = sorted([zeros2.iloc[j,1] for j in zeros2.index])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ones2_list = sorted([ones2.iloc[i,1] for i in ones2.index])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            zeros2_length = len(zeros2_list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ones2_length = len(ones2_list)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            conc = 0\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ties = 0\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            disc = 0\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            for i in zeros2.index:\n",
    "                cur_disc = bisect_left(ones2_list, zeros2.iloc[i,1])\n",
    "                cur_ties = bisect_right(ones2_list, zeros2.iloc[i,1]) - cur_disc\n",
    "                disc += cur_disc\n",
    "                ties += cur_ties\n",
    "                conc += ones2_length - cur_ties - cur_disc\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            pairs_tested = zeros2_length * ones2_length\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            concordance = conc/pairs_tested\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            discordance = disc/pairs_tested\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            ties_perc = ties/pairs_tested\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            print(\"Concordance = \", (concordance*100) , \"%\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            print(\"Discordance = \", (discordance*100) , \"%\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            print(\"Tied = \", (ties_perc*100) , \"%\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            print(\"Pairs = \", pairs_tested)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            return({\"Concordance\":concordance,'Discordance':discordance,'Ties':ties_perc})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error: Check if function parameters are correct and previous cells are executed\")\n",
    "\n",
    "\n",
    "    # ---------------------- to show feature importance --------------------\n",
    "    def show_features_imp(xgb_model=None,target=target):\n",
    "        dep_var = target\n",
    "        feat_imp_gbm=xgb_model.feature_importances_\n",
    "        train_new1 = train.drop([dep_var], axis = 1)\n",
    "        feature_gbm=train_new1.columns\n",
    "        dfnew = train.drop([dep_var], axis = 1)\n",
    "        feat_gbm=pd.DataFrame(feature_gbm,columns=['Feature'])\n",
    "        #feat_gbm[\"       \"] = dfnew.apply(lambda _: '>',axis=1)\n",
    "        feat_importance_gbm=pd.DataFrame(feat_imp_gbm,columns=['Feature Importance'])\n",
    "        df_feat_imp1_gbm = pd.concat([feat_gbm,feat_importance_gbm], axis=1)\n",
    "        df_feat_imp_gbm = df_feat_imp1_gbm.to_string(sparsify=bool)\n",
    "        print(df_feat_imp1_gbm.sort_values('Feature Importance',ascending=False))\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    # -----------------------------to create folder skleton ------------------------------\n",
    "    def create_skleton():\n",
    "        def get_uniq_model_run_name():\n",
    "            import time\n",
    "            uniq_name=f'Model_run_{time.strftime(\"%d_%b_%Y_%H_%M_%S\")}'\n",
    "            return uniq_name\n",
    "\n",
    "        model_run_path=os.path.join(os.getcwd(),get_uniq_model_run_name())\n",
    "        os.mkdir(model_run_path)\n",
    "        gain_path=os.path.join(model_run_path,'Gain')\n",
    "        os.mkdir(gain_path)\n",
    "        ks_path=os.path.join(model_run_path,'KS')\n",
    "        os.mkdir(ks_path)\n",
    "        sweetviz_path=os.path.join(model_run_path,'sweetviz_comparisons')\n",
    "        os.mkdir(sweetviz_path)\n",
    "        model_path=os.path.join(model_run_path,'Model')\n",
    "        os.mkdir(model_path)\n",
    "        file_store_path=os.path.join(model_run_path,'Data')\n",
    "        os.mkdir(file_store_path)\n",
    "\n",
    "\n",
    "        return gain_path,ks_path,sweetviz_path,model_path,file_store_path\n",
    "\n",
    "    # \n",
    "    #  # importing required libriries\n",
    "    logger = get_logger('model_managment')\n",
    "    logger.info('Importing required libraries')\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    import logging\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    from  sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    import os\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    import  sweetviz as sv\n",
    "\n",
    "    # ------------------------ setting index -------------------\n",
    "\n",
    "    logger.info('setting index started')\n",
    "    train.set_index(index_col,drop=True,inplace=True)\n",
    "\n",
    "\n",
    "    logger.info('splitting x y of train')\n",
    "    # splitting  train into x,y\n",
    "    x_train,y_train=train.drop(target,axis=1),train[target]\n",
    "\n",
    "    if parameter_optimization:\n",
    "        \n",
    "        logger.info('Hyperparameter tuning started')\n",
    "        xgb=XGBClassifier(objective='binary:logistic')\n",
    "        random_search=RandomizedSearchCV(xgb,n_jobs=-1,cv=CV,n_iter=n_iter,scoring=scoring,\n",
    "                                        param_distributions=param_grid)\n",
    "\n",
    "        random_search.fit(x_train,y_train)\n",
    "        print(report(random_search.cv_results_,show_n_report))\n",
    "        logger.info('Hyper parameter tuning completed')\n",
    "\n",
    "\n",
    "    logger.info(' model defining started')\n",
    "    if best_model and parameter_optimization:\n",
    "        model=random_search.best_estimator_\n",
    "    else:\n",
    "        logger .info('Model defining without hyper parameter tuning')\n",
    "        from xgboost import XGBClassifier\n",
    "        model=XGBClassifier(objective=objective,subsample=subsample,scale_pos_weight=scale_pos_weight,reg_lambda=reg_lambda,        reg_alpha=reg_alpha,n_estimators=n_estimators,\n",
    "                   min_child_weight=min_child_weight, max_depth=max_depth, max_delta_step=max_delta_step, learning_rate=learning_rate, gamma=gamma,\n",
    "                   colsample_bytree=colsample_bytree, colsample_bylevel=colsample_bylevel)\n",
    "\n",
    "        model.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    if show_feature_importance:\n",
    "        logger.info('Feature importance showing')\n",
    "        show_features_imp(xgb_model=model,target=target)\n",
    "    # ------------------- creating skleton---------------------\n",
    "    logger.info('Required folder creation started ')\n",
    "    gain_path,ks_path,sweetviz_path,model_path,file_store_path=create_skleton()\n",
    "\n",
    "    # ----------------------saving model ----------------------\n",
    "    logger.info('model saving started')\n",
    "    \n",
    "    filename = model_name+'.pkl'\n",
    "    pickle.dump(model, open(os.path.join(model_path,filename), 'wb'))\n",
    "    logger.info('model saving done')\n",
    "\n",
    "        \n",
    "\n",
    "    # ------------------------------ predictions --------------------------------\n",
    "    print('--------------------train performance ----------------')\n",
    "    logger.info('Prediction for train started')\n",
    "    train[\"Pred_Prob\"]=model.predict_proba(x_train)[:,1]\n",
    "    train.to_csv(os.path.join(file_store_path,'train_xgb.csv'))\n",
    "    logger.info('train file saved')\n",
    "    # getConcordance(train,target)\n",
    "       #----------------------- for train \n",
    "    # -- gain\n",
    "    logger.info('Gain for train month started')\n",
    "    gains_train = createGains(train,target)\n",
    "    gains_table_train=gains_train[0]\n",
    "    print('--------- Train gain-------')\n",
    "    print(gains_table_train) \n",
    "    gains_table_train.to_csv(os.path.join(gain_path,'train_GAIN.csv'))\n",
    "\n",
    "    #-- ks\n",
    "    logger.info('KS for train month started')\n",
    "    oot_apr_log1 = train.reset_index(drop=True)\n",
    "    ks_rf_oot_apr = getKS(oot_apr_log1,target)\n",
    "    ks_rf_oot_apr[0].to_csv(os.path.join(ks_path,'train_KS.csv'))\n",
    "    \n",
    "    # concordance\n",
    "    \n",
    "\n",
    "\n",
    "    print('performance and prediction for tetst')\n",
    "    if tests:\n",
    "        for i in range(len(tests)):\n",
    "            \n",
    "            # print('im here')\n",
    "            test_df=tests[i]\n",
    "            test_df_name=test_file_names[i]\n",
    "            logger.info(f'{test_df_name} report generation started')\n",
    "            print(f'<<<<<<<<<<<<<<<<-------{test_df_name}------->>>>>>>>>>>>>>')\n",
    "            # print(test_df.columns)\n",
    "            test_df.set_index(index_col,drop=True,inplace=True)\n",
    "            logger.info(f'prediction for {test_df_name} ')\n",
    "            test_df[\"Pred_Prob\"]=model.predict_proba(test_df.drop(target,axis=1))[:,1]\n",
    "            print(os.path.join(file_store_path,f'{test_df_name}_xgb.csv'))\n",
    "            test_df.to_csv(os.path.join(file_store_path,f'{test_df_name}_xgb.csv'))\n",
    "            logger.info(f'{test_df_name} file saved')\n",
    "            # print('prediction_done')\n",
    "            logger.info(f' gain for {test_df_name} started')\n",
    "            gains_train = createGains(test_df,target)\n",
    "            gains_table_train=gains_train[0]\n",
    "            print('--------- Train gain-------')\n",
    "            print(gains_table_train) \n",
    "            gain_file_name=f'{test_df_name}_GAIN.csv'\n",
    "            gains_table_train.to_csv(os.path.join(gain_path,gain_file_name))\n",
    "            logger.info(f' gain for {test_df_name} saved')\n",
    "            #-- ks\n",
    "            logger.info(f' KS for {test_df_name} started')\n",
    "            oot_apr_log1 = test_df.reset_index(drop=True)\n",
    "            ks_rf_oot_apr = getKS(oot_apr_log1,target)\n",
    "            ks_file_name=f'{test_df_name}_KS.csv'\n",
    "            ks_rf_oot_apr[0].to_csv(os.path.join(ks_path,ks_file_name))\n",
    "            logger.info(f' KS for {test_df_name} file saved')\n",
    "\n",
    "            # concordance discordance\n",
    "            # getConcordance(test_df,target)\n",
    "\n",
    "\n",
    "            # sweetviz comparison\n",
    "            logger.info(f'sweetviz comparison for train and {test_df_name} started')\n",
    "            \n",
    "            compare_report1 = sv.compare([train, 'Train'], [test_df, test_df_name])\n",
    "            sweetviz_file_name=f'train_{test_df_name} compare.html'\n",
    "            compare_report1.show_html(os.path.join(sweetviz_path,sweetviz_file_name), open_browser=False)\n",
    "            logger.info(f'sweetviz comparison for train and {test_df_name} file saved')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "train=train.iloc[:500,:]\n",
    "aug21=pd.read_csv('aug21.csv')\n",
    "aug21=aug21.iloc[:200,:]\n",
    "jan22=pd.read_csv('jan22.csv')\n",
    "jan22=jan22.iloc[:200,:]\n",
    "jul22=pd.read_csv('jul22.csv')\n",
    "jul22=jul22.iloc[:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_trail.csv')\n",
    "aug21.to_csv('aug21_trial.csv')\n",
    "jan22.to_csv('jan22_trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 22:50:28,749  model_managment   INFO  Importing required libraries at line no 685\n",
      "2022-08-27 22:50:28,749  model_managment   INFO  Importing required libraries at line no 685\n",
      "2022-08-27 22:50:28,749  model_managment   INFO  Importing required libraries at line no 685\n",
      "2022-08-27 22:50:28,749  model_managment   INFO  setting index started at line no 705\n",
      "2022-08-27 22:50:28,749  model_managment   INFO  setting index started at line no 705\n",
      "2022-08-27 22:50:28,749  model_managment   INFO  setting index started at line no 705\n",
      "2022-08-27 22:50:28,757  model_managment   INFO  splitting x y of train at line no 709\n",
      "2022-08-27 22:50:28,757  model_managment   INFO  splitting x y of train at line no 709\n",
      "2022-08-27 22:50:28,757  model_managment   INFO  splitting x y of train at line no 709\n",
      "2022-08-27 22:50:28,757  model_managment   INFO   model defining started at line no 725\n",
      "2022-08-27 22:50:28,757  model_managment   INFO   model defining started at line no 725\n",
      "2022-08-27 22:50:28,757  model_managment   INFO   model defining started at line no 725\n",
      "2022-08-27 22:50:28,757  model_managment   INFO  Model defining without hyper parameter tuning at line no 729\n",
      "2022-08-27 22:50:28,757  model_managment   INFO  Model defining without hyper parameter tuning at line no 729\n",
      "2022-08-27 22:50:28,757  model_managment   INFO  Model defining without hyper parameter tuning at line no 729\n",
      "2022-08-27 22:50:28,799  model_managment   INFO  Required folder creation started  at line no 742\n",
      "2022-08-27 22:50:28,799  model_managment   INFO  Required folder creation started  at line no 742\n",
      "2022-08-27 22:50:28,799  model_managment   INFO  Required folder creation started  at line no 742\n",
      "2022-08-27 22:50:28,807  model_managment   INFO  model saving started at line no 746\n",
      "2022-08-27 22:50:28,807  model_managment   INFO  model saving started at line no 746\n",
      "2022-08-27 22:50:28,807  model_managment   INFO  model saving started at line no 746\n",
      "2022-08-27 22:50:28,814  model_managment   INFO  model saving done at line no 750\n",
      "2022-08-27 22:50:28,814  model_managment   INFO  model saving done at line no 750\n",
      "2022-08-27 22:50:28,814  model_managment   INFO  model saving done at line no 750\n",
      "2022-08-27 22:50:28,815  model_managment   INFO  Prediction for train started at line no 756\n",
      "2022-08-27 22:50:28,815  model_managment   INFO  Prediction for train started at line no 756\n",
      "2022-08-27 22:50:28,815  model_managment   INFO  Prediction for train started at line no 756\n",
      "2022-08-27 22:50:28,836  model_managment   INFO  train file saved at line no 759\n",
      "2022-08-27 22:50:28,836  model_managment   INFO  train file saved at line no 759\n",
      "2022-08-27 22:50:28,836  model_managment   INFO  train file saved at line no 759\n",
      "2022-08-27 22:50:28,837  model_managment   INFO  Gain for train month started at line no 763\n",
      "2022-08-27 22:50:28,837  model_managment   INFO  Gain for train month started at line no 763\n",
      "2022-08-27 22:50:28,837  model_managment   INFO  Gain for train month started at line no 763\n",
      "2022-08-27 22:50:28,910  model_managment   INFO  KS for train month started at line no 771\n",
      "2022-08-27 22:50:28,910  model_managment   INFO  KS for train month started at line no 771\n",
      "2022-08-27 22:50:28,910  model_managment   INFO  KS for train month started at line no 771\n",
      "2022-08-27 22:50:28,982  model_managment   INFO  aug21 report generation started at line no 787\n",
      "2022-08-27 22:50:28,982  model_managment   INFO  aug21 report generation started at line no 787\n",
      "2022-08-27 22:50:28,982  model_managment   INFO  aug21 report generation started at line no 787\n",
      "2022-08-27 22:50:28,982  model_managment   INFO  prediction for aug21  at line no 791\n",
      "2022-08-27 22:50:28,982  model_managment   INFO  prediction for aug21  at line no 791\n",
      "2022-08-27 22:50:28,982  model_managment   INFO  prediction for aug21  at line no 791\n",
      "2022-08-27 22:50:28,993  model_managment   INFO  aug21 file saved at line no 795\n",
      "2022-08-27 22:50:28,993  model_managment   INFO  aug21 file saved at line no 795\n",
      "2022-08-27 22:50:28,993  model_managment   INFO  aug21 file saved at line no 795\n",
      "2022-08-27 22:50:28,999  model_managment   INFO   gain for aug21 started at line no 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------train performance ----------------\n",
      "'DataFrame' object has no attribute 'is_copy'\n",
      "--------- Train gain-------\n",
      "   min_score max_scr DepVar Non_Event total Cumulative_Sum Perc_of_Events  \\\n",
      "0          0       0      0         0     0              0              0   \n",
      "1      0.334   0.991     42         8    50             42           84.0   \n",
      "2      0.082   0.334      6        44    50             48           12.0   \n",
      "3      0.021   0.082      1        49    50             49            2.0   \n",
      "4      0.004   0.021      0        50    50             49            0.0   \n",
      "5      0.004   0.004      0        50    50             49            0.0   \n",
      "6      0.004   0.004      1        49    50             50            2.0   \n",
      "7      0.004   0.004      0        50    50             50            0.0   \n",
      "8      0.004   0.004      0        50    50             50            0.0   \n",
      "9      0.004   0.004      0        50    50             50            0.0   \n",
      "10     0.002   0.004      0        50    50             50            0.0   \n",
      "\n",
      "     Gain Bucket  \n",
      "0       0      0  \n",
      "1    84.0      1  \n",
      "2    96.0      2  \n",
      "3    98.0      3  \n",
      "4    98.0      4  \n",
      "5    98.0      5  \n",
      "6   100.0      6  \n",
      "7   100.0      7  \n",
      "8   100.0      8  \n",
      "9   100.0      9  \n",
      "10  100.0     10  \n",
      "'DataFrame' object has no attribute 'is_copy'\n",
      "        min_score   max_scr  Event  NonEvent  total  Cumulative_Sum_Event  \\\n",
      "Bucket                                                                      \n",
      "1        0.333790  0.990724     42         8     50                    42   \n",
      "2        0.081896  0.333790      6        44     50                    48   \n",
      "3        0.021334  0.081896      1        49     50                    49   \n",
      "4        0.004037  0.021334      0        50     50                    49   \n",
      "5        0.003791  0.004037      0        50     50                    49   \n",
      "6        0.003791  0.003791      1        49     50                    50   \n",
      "7        0.003791  0.003791      0        50     50                    50   \n",
      "8        0.003791  0.003791      0        50     50                    50   \n",
      "9        0.003791  0.003791      0        50     50                    50   \n",
      "10       0.001692  0.003791      0        50     50                    50   \n",
      "\n",
      "        Perc_of_Events  Cumulative_Percent_Event  Cumulative_Sum_NonEvent  \\\n",
      "Bucket                                                                      \n",
      "1                 84.0                      84.0                        8   \n",
      "2                 12.0                      96.0                       52   \n",
      "3                  2.0                      98.0                      101   \n",
      "4                  0.0                      98.0                      151   \n",
      "5                  0.0                      98.0                      201   \n",
      "6                  2.0                     100.0                      250   \n",
      "7                  0.0                     100.0                      300   \n",
      "8                  0.0                     100.0                      350   \n",
      "9                  0.0                     100.0                      400   \n",
      "10                 0.0                     100.0                      450   \n",
      "\n",
      "        Perc_of_NonEvents  Cumulative_Percent_NonEvent    ks      max_ks  \n",
      "Bucket                                                                    \n",
      "1                1.777778                     1.777778  82.0              \n",
      "2                9.777778                    11.555556  84.0  <----(max)  \n",
      "3               10.888889                    22.444444  76.0              \n",
      "4               11.111111                    33.555556  64.0              \n",
      "5               11.111111                    44.666667  53.0              \n",
      "6               10.888889                    55.555556  44.0              \n",
      "7               11.111111                    66.666667  33.0              \n",
      "8               11.111111                    77.777778  22.0              \n",
      "9               11.111111                    88.888889  11.0              \n",
      "10              11.111111                   100.000000  -0.0              \n",
      "performance and prediction for tetst\n",
      "<<<<<<<<<<<<<<<<-------aug21------->>>>>>>>>>>>>>\n",
      "c:\\Users\\5053226\\Desktop\\sample_practice\\my_PY_PI_packages\\XGB_binary\\Model_run_27_Aug_2022_22_50_28\\Data\\aug21_xgb.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 22:50:28,999  model_managment   INFO   gain for aug21 started at line no 797\n",
      "2022-08-27 22:50:28,999  model_managment   INFO   gain for aug21 started at line no 797\n",
      "2022-08-27 22:50:29,073  model_managment   INFO   gain for aug21 saved at line no 804\n",
      "2022-08-27 22:50:29,073  model_managment   INFO   gain for aug21 saved at line no 804\n",
      "2022-08-27 22:50:29,073  model_managment   INFO   gain for aug21 saved at line no 804\n",
      "2022-08-27 22:50:29,074  model_managment   INFO   KS for aug21 started at line no 806\n",
      "2022-08-27 22:50:29,074  model_managment   INFO   KS for aug21 started at line no 806\n",
      "2022-08-27 22:50:29,074  model_managment   INFO   KS for aug21 started at line no 806\n",
      "2022-08-27 22:50:29,157  model_managment   INFO   KS for aug21 file saved at line no 811\n",
      "2022-08-27 22:50:29,157  model_managment   INFO   KS for aug21 file saved at line no 811\n",
      "2022-08-27 22:50:29,157  model_managment   INFO   KS for aug21 file saved at line no 811\n",
      "2022-08-27 22:50:29,157  model_managment   INFO  sweetviz comparison for train and aug21 started at line no 818\n",
      "2022-08-27 22:50:29,157  model_managment   INFO  sweetviz comparison for train and aug21 started at line no 818\n",
      "2022-08-27 22:50:29,157  model_managment   INFO  sweetviz comparison for train and aug21 started at line no 818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DataFrame' object has no attribute 'is_copy'\n",
      "--------- Train gain-------\n",
      "   min_score max_scr DepVar Non_Event total Cumulative_Sum Perc_of_Events  \\\n",
      "0          0       0      0         0     0              0              0   \n",
      "1      0.801   0.986     14         6    20             14          43.75   \n",
      "2      0.281   0.801     11         9    20             25         34.375   \n",
      "3      0.102   0.253      0        20    20             25            0.0   \n",
      "4      0.082   0.102      3        17    20             28          9.375   \n",
      "5      0.013   0.082      4        16    20             32           12.5   \n",
      "6      0.004   0.013      0        20    20             32            0.0   \n",
      "7      0.004   0.004      0        20    20             32            0.0   \n",
      "8      0.004   0.004      0        20    20             32            0.0   \n",
      "9      0.002   0.004      0        20    20             32            0.0   \n",
      "10     0.002   0.002      0        20    20             32            0.0   \n",
      "\n",
      "      Gain Bucket  \n",
      "0        0      0  \n",
      "1    43.75      1  \n",
      "2   78.125      2  \n",
      "3   78.125      3  \n",
      "4     87.5      4  \n",
      "5    100.0      5  \n",
      "6    100.0      6  \n",
      "7    100.0      7  \n",
      "8    100.0      8  \n",
      "9    100.0      9  \n",
      "10   100.0     10  \n",
      "'DataFrame' object has no attribute 'is_copy'\n",
      "        min_score   max_scr  Event  NonEvent  total  Cumulative_Sum_Event  \\\n",
      "Bucket                                                                      \n",
      "1        0.801272  0.986216     14         6     20                    14   \n",
      "2        0.280549  0.801272     11         9     20                    25   \n",
      "3        0.101509  0.252967      0        20     20                    25   \n",
      "4        0.081896  0.101509      3        17     20                    28   \n",
      "5        0.013054  0.081896      4        16     20                    32   \n",
      "6        0.003791  0.013054      0        20     20                    32   \n",
      "7        0.003791  0.003791      0        20     20                    32   \n",
      "8        0.003791  0.003791      0        20     20                    32   \n",
      "9        0.001783  0.003674      0        20     20                    32   \n",
      "10       0.001692  0.001783      0        20     20                    32   \n",
      "\n",
      "        Perc_of_Events  Cumulative_Percent_Event  Cumulative_Sum_NonEvent  \\\n",
      "Bucket                                                                      \n",
      "1               43.750                    43.750                        6   \n",
      "2               34.375                    78.125                       15   \n",
      "3                0.000                    78.125                       35   \n",
      "4                9.375                    87.500                       52   \n",
      "5               12.500                   100.000                       68   \n",
      "6                0.000                   100.000                       88   \n",
      "7                0.000                   100.000                      108   \n",
      "8                0.000                   100.000                      128   \n",
      "9                0.000                   100.000                      148   \n",
      "10               0.000                   100.000                      168   \n",
      "\n",
      "        Perc_of_NonEvents  Cumulative_Percent_NonEvent    ks      max_ks  \n",
      "Bucket                                                                    \n",
      "1                3.571429                     3.571429  40.0              \n",
      "2                5.357143                     8.928571  69.0  <----(max)  \n",
      "3               11.904762                    20.833333  57.0              \n",
      "4               10.119048                    30.952381  57.0              \n",
      "5                9.523810                    40.476190  60.0              \n",
      "6               11.904762                    52.380952  48.0              \n",
      "7               11.904762                    64.285714  36.0              \n",
      "8               11.904762                    76.190476  24.0              \n",
      "9               11.904762                    88.095238  12.0              \n",
      "10              11.904762                   100.000000   0.0              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -> (00:00 left)    \n",
      "2022-08-27 22:50:32,939  model_managment   INFO  sweetviz comparison for train and aug21 file saved at line no 823\n",
      "2022-08-27 22:50:32,939  model_managment   INFO  sweetviz comparison for train and aug21 file saved at line no 823\n",
      "2022-08-27 22:50:32,939  model_managment   INFO  sweetviz comparison for train and aug21 file saved at line no 823\n",
      "2022-08-27 22:50:32,947  model_managment   INFO  jan22 report generation started at line no 787\n",
      "2022-08-27 22:50:32,947  model_managment   INFO  jan22 report generation started at line no 787\n",
      "2022-08-27 22:50:32,947  model_managment   INFO  jan22 report generation started at line no 787\n",
      "2022-08-27 22:50:32,947  model_managment   INFO  prediction for jan22  at line no 791\n",
      "2022-08-27 22:50:32,947  model_managment   INFO  prediction for jan22  at line no 791\n",
      "2022-08-27 22:50:32,947  model_managment   INFO  prediction for jan22  at line no 791\n",
      "2022-08-27 22:50:32,971  model_managment   INFO  jan22 file saved at line no 795\n",
      "2022-08-27 22:50:32,971  model_managment   INFO  jan22 file saved at line no 795\n",
      "2022-08-27 22:50:32,971  model_managment   INFO  jan22 file saved at line no 795\n",
      "2022-08-27 22:50:32,971  model_managment   INFO   gain for jan22 started at line no 797\n",
      "2022-08-27 22:50:32,971  model_managment   INFO   gain for jan22 started at line no 797\n",
      "2022-08-27 22:50:32,971  model_managment   INFO   gain for jan22 started at line no 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report c:\\Users\\5053226\\Desktop\\sample_practice\\my_PY_PI_packages\\XGB_binary\\Model_run_27_Aug_2022_22_50_28\\sweetviz_comparisons\\train_aug21 compare.html was generated.\n",
      "<<<<<<<<<<<<<<<<-------jan22------->>>>>>>>>>>>>>\n",
      "c:\\Users\\5053226\\Desktop\\sample_practice\\my_PY_PI_packages\\XGB_binary\\Model_run_27_Aug_2022_22_50_28\\Data\\jan22_xgb.csv\n",
      "'DataFrame' object has no attribute 'is_copy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 22:50:33,173  model_managment   INFO   gain for jan22 saved at line no 804\n",
      "2022-08-27 22:50:33,173  model_managment   INFO   gain for jan22 saved at line no 804\n",
      "2022-08-27 22:50:33,173  model_managment   INFO   gain for jan22 saved at line no 804\n",
      "2022-08-27 22:50:33,173  model_managment   INFO   KS for jan22 started at line no 806\n",
      "2022-08-27 22:50:33,173  model_managment   INFO   KS for jan22 started at line no 806\n",
      "2022-08-27 22:50:33,173  model_managment   INFO   KS for jan22 started at line no 806\n",
      "2022-08-27 22:50:33,238  model_managment   INFO   KS for jan22 file saved at line no 811\n",
      "2022-08-27 22:50:33,238  model_managment   INFO   KS for jan22 file saved at line no 811\n",
      "2022-08-27 22:50:33,238  model_managment   INFO   KS for jan22 file saved at line no 811\n",
      "2022-08-27 22:50:33,238  model_managment   INFO  sweetviz comparison for train and jan22 started at line no 818\n",
      "2022-08-27 22:50:33,238  model_managment   INFO  sweetviz comparison for train and jan22 started at line no 818\n",
      "2022-08-27 22:50:33,238  model_managment   INFO  sweetviz comparison for train and jan22 started at line no 818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Train gain-------\n",
      "   min_score max_scr DepVar Non_Event total Cumulative_Sum Perc_of_Events  \\\n",
      "0          0       0      0         0     0              0              0   \n",
      "1      0.593   0.986     15         5    20             15           75.0   \n",
      "2      0.183    0.53      3        17    20             18           15.0   \n",
      "3      0.088   0.183      1        19    20             19            5.0   \n",
      "4      0.034   0.088      0        20    20             19            0.0   \n",
      "5      0.013   0.024      0        20    20             19            0.0   \n",
      "6      0.004   0.013      0        20    20             19            0.0   \n",
      "7      0.004   0.004      0        20    20             19            0.0   \n",
      "8      0.004   0.004      0        20    20             19            0.0   \n",
      "9      0.002   0.004      1        19    20             20            5.0   \n",
      "10     0.002   0.002      0        20    20             20            0.0   \n",
      "\n",
      "     Gain Bucket  \n",
      "0       0      0  \n",
      "1    75.0      1  \n",
      "2    90.0      2  \n",
      "3    95.0      3  \n",
      "4    95.0      4  \n",
      "5    95.0      5  \n",
      "6    95.0      6  \n",
      "7    95.0      7  \n",
      "8    95.0      8  \n",
      "9   100.0      9  \n",
      "10  100.0     10  \n",
      "'DataFrame' object has no attribute 'is_copy'\n",
      "        min_score   max_scr  Event  NonEvent  total  Cumulative_Sum_Event  \\\n",
      "Bucket                                                                      \n",
      "1        0.593076  0.986216     15         5     20                    15   \n",
      "2        0.182818  0.530089      3        17     20                    18   \n",
      "3        0.087782  0.182818      1        19     20                    19   \n",
      "4        0.033805  0.087782      0        20     20                    19   \n",
      "5        0.013054  0.024287      0        20     20                    19   \n",
      "6        0.003791  0.013054      0        20     20                    19   \n",
      "7        0.003791  0.003791      0        20     20                    19   \n",
      "8        0.003674  0.003791      0        20     20                    19   \n",
      "9        0.001783  0.003674      1        19     20                    20   \n",
      "10       0.001692  0.001783      0        20     20                    20   \n",
      "\n",
      "        Perc_of_Events  Cumulative_Percent_Event  Cumulative_Sum_NonEvent  \\\n",
      "Bucket                                                                      \n",
      "1                 75.0                      75.0                        5   \n",
      "2                 15.0                      90.0                       22   \n",
      "3                  5.0                      95.0                       41   \n",
      "4                  0.0                      95.0                       61   \n",
      "5                  0.0                      95.0                       81   \n",
      "6                  0.0                      95.0                      101   \n",
      "7                  0.0                      95.0                      121   \n",
      "8                  0.0                      95.0                      141   \n",
      "9                  5.0                     100.0                      160   \n",
      "10                 0.0                     100.0                      180   \n",
      "\n",
      "        Perc_of_NonEvents  Cumulative_Percent_NonEvent    ks      max_ks  \n",
      "Bucket                                                                    \n",
      "1                2.777778                     2.777778  72.0              \n",
      "2                9.444444                    12.222222  78.0  <----(max)  \n",
      "3               10.555556                    22.777778  72.0              \n",
      "4               11.111111                    33.888889  61.0              \n",
      "5               11.111111                    45.000000  50.0              \n",
      "6               11.111111                    56.111111  39.0              \n",
      "7               11.111111                    67.222222  28.0              \n",
      "8               11.111111                    78.333333  17.0              \n",
      "9               10.555556                    88.888889  11.0              \n",
      "10              11.111111                   100.000000  -0.0              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -> (00:00 left)    \n",
      "2022-08-27 22:50:36,540  model_managment   INFO  sweetviz comparison for train and jan22 file saved at line no 823\n",
      "2022-08-27 22:50:36,540  model_managment   INFO  sweetviz comparison for train and jan22 file saved at line no 823\n",
      "2022-08-27 22:50:36,540  model_managment   INFO  sweetviz comparison for train and jan22 file saved at line no 823\n",
      "2022-08-27 22:50:36,540  model_managment   INFO  jul22 report generation started at line no 787\n",
      "2022-08-27 22:50:36,540  model_managment   INFO  jul22 report generation started at line no 787\n",
      "2022-08-27 22:50:36,540  model_managment   INFO  jul22 report generation started at line no 787\n",
      "2022-08-27 22:50:36,548  model_managment   INFO  prediction for jul22  at line no 791\n",
      "2022-08-27 22:50:36,548  model_managment   INFO  prediction for jul22  at line no 791\n",
      "2022-08-27 22:50:36,548  model_managment   INFO  prediction for jul22  at line no 791\n",
      "2022-08-27 22:50:36,557  model_managment   INFO  jul22 file saved at line no 795\n",
      "2022-08-27 22:50:36,557  model_managment   INFO  jul22 file saved at line no 795\n",
      "2022-08-27 22:50:36,557  model_managment   INFO  jul22 file saved at line no 795\n",
      "2022-08-27 22:50:36,566  model_managment   INFO   gain for jul22 started at line no 797\n",
      "2022-08-27 22:50:36,566  model_managment   INFO   gain for jul22 started at line no 797\n",
      "2022-08-27 22:50:36,566  model_managment   INFO   gain for jul22 started at line no 797\n",
      "2022-08-27 22:50:36,645  model_managment   INFO   gain for jul22 saved at line no 804\n",
      "2022-08-27 22:50:36,645  model_managment   INFO   gain for jul22 saved at line no 804\n",
      "2022-08-27 22:50:36,645  model_managment   INFO   gain for jul22 saved at line no 804\n",
      "2022-08-27 22:50:36,646  model_managment   INFO   KS for jul22 started at line no 806\n",
      "2022-08-27 22:50:36,646  model_managment   INFO   KS for jul22 started at line no 806\n",
      "2022-08-27 22:50:36,646  model_managment   INFO   KS for jul22 started at line no 806\n",
      "2022-08-27 22:50:36,704  model_managment   INFO   KS for jul22 file saved at line no 811\n",
      "2022-08-27 22:50:36,704  model_managment   INFO   KS for jul22 file saved at line no 811\n",
      "2022-08-27 22:50:36,704  model_managment   INFO   KS for jul22 file saved at line no 811\n",
      "2022-08-27 22:50:36,712  model_managment   INFO  sweetviz comparison for train and jul22 started at line no 818\n",
      "2022-08-27 22:50:36,712  model_managment   INFO  sweetviz comparison for train and jul22 started at line no 818\n",
      "2022-08-27 22:50:36,712  model_managment   INFO  sweetviz comparison for train and jul22 started at line no 818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report c:\\Users\\5053226\\Desktop\\sample_practice\\my_PY_PI_packages\\XGB_binary\\Model_run_27_Aug_2022_22_50_28\\sweetviz_comparisons\\train_jan22 compare.html was generated.\n",
      "<<<<<<<<<<<<<<<<-------jul22------->>>>>>>>>>>>>>\n",
      "c:\\Users\\5053226\\Desktop\\sample_practice\\my_PY_PI_packages\\XGB_binary\\Model_run_27_Aug_2022_22_50_28\\Data\\jul22_xgb.csv\n",
      "'DataFrame' object has no attribute 'is_copy'\n",
      "--------- Train gain-------\n",
      "   min_score max_scr DepVar Non_Event total Cumulative_Sum Perc_of_Events  \\\n",
      "0          0       0      0         0     0              0              0   \n",
      "1      0.334   0.986      8        12    20              8         53.333   \n",
      "2      0.102   0.281      1        19    20              9          6.667   \n",
      "3      0.088   0.094      2        18    20             11         13.333   \n",
      "4      0.064   0.088      3        17    20             14           20.0   \n",
      "5      0.013   0.027      1        19    20             15          6.667   \n",
      "6      0.004   0.013      0        20    20             15            0.0   \n",
      "7      0.004   0.004      0        20    20             15            0.0   \n",
      "8      0.004   0.004      0        20    20             15            0.0   \n",
      "9      0.002   0.004      0        20    20             15            0.0   \n",
      "10     0.002   0.002      0        20    20             15            0.0   \n",
      "\n",
      "      Gain Bucket  \n",
      "0        0      0  \n",
      "1   53.333      1  \n",
      "2     60.0      2  \n",
      "3   73.333      3  \n",
      "4   93.333      4  \n",
      "5    100.0      5  \n",
      "6    100.0      6  \n",
      "7    100.0      7  \n",
      "8    100.0      8  \n",
      "9    100.0      9  \n",
      "10   100.0     10  \n",
      "'DataFrame' object has no attribute 'is_copy'\n",
      "        min_score   max_scr  Event  NonEvent  total  Cumulative_Sum_Event  \\\n",
      "Bucket                                                                      \n",
      "1        0.333790  0.986216      8        12     20                     8   \n",
      "2        0.101509  0.280549      1        19     20                     9   \n",
      "3        0.087782  0.093812      2        18     20                    11   \n",
      "4        0.063907  0.087782      3        17     20                    14   \n",
      "5        0.013054  0.026639      1        19     20                    15   \n",
      "6        0.003791  0.013054      0        20     20                    15   \n",
      "7        0.003791  0.003791      0        20     20                    15   \n",
      "8        0.003791  0.003791      0        20     20                    15   \n",
      "9        0.001783  0.003791      0        20     20                    15   \n",
      "10       0.001692  0.001783      0        20     20                    15   \n",
      "\n",
      "        Perc_of_Events  Cumulative_Percent_Event  Cumulative_Sum_NonEvent  \\\n",
      "Bucket                                                                      \n",
      "1            53.333333                 53.333333                       12   \n",
      "2             6.666667                 60.000000                       31   \n",
      "3            13.333333                 73.333333                       49   \n",
      "4            20.000000                 93.333333                       66   \n",
      "5             6.666667                100.000000                       85   \n",
      "6             0.000000                100.000000                      105   \n",
      "7             0.000000                100.000000                      125   \n",
      "8             0.000000                100.000000                      145   \n",
      "9             0.000000                100.000000                      165   \n",
      "10            0.000000                100.000000                      185   \n",
      "\n",
      "        Perc_of_NonEvents  Cumulative_Percent_NonEvent    ks      max_ks  \n",
      "Bucket                                                                    \n",
      "1                6.486486                     6.486486  47.0              \n",
      "2               10.270270                    16.756757  43.0              \n",
      "3                9.729730                    26.486486  47.0              \n",
      "4                9.189189                    35.675676  58.0  <----(max)  \n",
      "5               10.270270                    45.945946  54.0              \n",
      "6               10.810811                    56.756757  43.0              \n",
      "7               10.810811                    67.567568  32.0              \n",
      "8               10.810811                    78.378378  22.0              \n",
      "9               10.810811                    89.189189  11.0              \n",
      "10              10.810811                   100.000000   0.0              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:01 -> (00:00 left)    \n",
      "2022-08-27 22:50:40,088  model_managment   INFO  sweetviz comparison for train and jul22 file saved at line no 823\n",
      "2022-08-27 22:50:40,088  model_managment   INFO  sweetviz comparison for train and jul22 file saved at line no 823\n",
      "2022-08-27 22:50:40,088  model_managment   INFO  sweetviz comparison for train and jul22 file saved at line no 823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report c:\\Users\\5053226\\Desktop\\sample_practice\\my_PY_PI_packages\\XGB_binary\\Model_run_27_Aug_2022_22_50_28\\sweetviz_comparisons\\train_jul22 compare.html was generated.\n"
     ]
    }
   ],
   "source": [
    "xgb_binary_classifier(train=train,\n",
    "                        tests=[aug21,jan22,jul22],\n",
    "                        test_file_names=['aug21','jan22','jul22'],\n",
    "                        target='PERF_Y_DEF_RF_FLAG',\n",
    "                        index_col='ID_NEW',\n",
    "                        model_name='cspl_rof',\n",
    "                        parameter_optimization=False,\n",
    "                        show_n_report=5,\n",
    "                        show_feature_importance=False,\n",
    "                        param_grid={  \n",
    "                                    \"learning_rate\":[.01,.015,.02,.025,.03,.035,.04,.045,.05],\n",
    "                                    \"gamma\":[i/10.0 for i in range(0,5)],\n",
    "                                    \"max_depth\": [2,3,4,5,6,7,8],\n",
    "                                    \"min_child_weight\":[1,2,5,10],\n",
    "                                    \"max_delta_step\":[0,1,2,5,10],\n",
    "                                    \"subsample\":[i/10.0 for i in range(5,10)],\n",
    "                                    \"colsample_bytree\":[i/10.0 for i in range(5,10)],\n",
    "                                    \"colsample_bylevel\":[i/10.0 for i in range(5,10)],\n",
    "                                    \"reg_lambda\":[1e-5, 1e-2, 0.1, 1, 100], \n",
    "                                    \"reg_alpha\":[1e-5, 1e-2, 0.1, 1, 100],\n",
    "                                    \"scale_pos_weight\":[1,2,3,4,5,6,7,8,9,10,20,30,40],\n",
    "                                    \"n_estimators\":[100,500,700,1000]},\n",
    "                        n_iter=10,\n",
    "                        CV=5,\n",
    "                        scoring='roc_auc',\n",
    "                        best_model=True,\n",
    "                        learning_rate=0.1,\n",
    "                        gamma=0,\n",
    "                        max_depth=3,\n",
    "                        min_child_weight=1,\n",
    "                        max_delta_step=0,\n",
    "                        subsample=1,\n",
    "                        colsample_bytree=1,\n",
    "                        colsample_bylevel=1,\n",
    "                        reg_lambda=1,\n",
    "                        reg_alpha=0,\n",
    "                        scale_pos_weight=1,\n",
    "                        n_estimators=100,\n",
    "                        objective='binary:logistic'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skleton():\n",
    "    def get_uniq_model_run_name():\n",
    "        import time\n",
    "        uniq_name=f'Model_run_{time.strftime(\"%d_%b_%Y_%H_%M_%S\")}'\n",
    "        return uniq_name\n",
    "\n",
    "    model_run_path=os.path.join(os.getcwd(),get_uniq_model_run_name())\n",
    "    os.mkdir(model_run_path)\n",
    "    gain_path=os.path.join(model_run_path,'Gain')\n",
    "    os.mkdir(gain_path)\n",
    "    ks_path=os.path.join(model_run_path,'KS')\n",
    "    os.mkdir(ks_path)\n",
    "    sweetviz_path=os.path.join(model_run_path,'sweetviz_comparisons')\n",
    "    os.mkdir(sweetviz_path)\n",
    "    model_path=os.path.join(model_run_path,'Model')\n",
    "    os.mkdir(model_path)\n",
    "    return gain_path,ks_path,sweetviz_path,model_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniq_model_run_name():\n",
    "    import time\n",
    "    uniq_name=f'Model_run_{time.strftime(\"%d_%b_%Y_%H_%M_%S\")}'\n",
    "    return uniq_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(get_uniq_model_run_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model_run_27_Aug_2022_10_29_09'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_uniq_model_run_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27_Aug_2022_10_28_18'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.strftime('%d_%b_%Y_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('binary_classifier_packeges')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb18bb24d533904f9c38d0ae7a50c45e9683bf77506d6dc1a74c650d819e017b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
